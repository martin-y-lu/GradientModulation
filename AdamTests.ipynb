{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fa81d6f-9072-43ae-bd9b-c3f404a48a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57562967-17aa-4716-99bc-56e5d02437dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (GModReLU.py, line 141)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/opt/anaconda3/envs/torchenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3579\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 2\u001b[0;36m\n\u001b[0;31m    import GModReLU as GModReLUMod\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Desktop/GitHub/ai/GradientModulation/GModReLU.py:141\u001b[0;36m\u001b[0m\n\u001b[0;31m    grad_weight = lin_grad_output.T @ input +\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import GModReLU as GModReLUMod\n",
    "importlib.reload(GModReLUMod)\n",
    "from GModReLU import GModReLU, GModReLUFunction, LGRLinear,LGRConv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "904e9276-faaa-48d4-8224-072821260c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transform\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "full_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Split into train and val\n",
    "train_size = int(0.9 * len(full_train))\n",
    "val_size = len(full_train) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f828afb5-099c-46cc-bc4a-ec7c49a15124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LGRCNN(nn.Module):\n",
    "    def __init__(self, l=0.01, k=5.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = LGRConv2d(3, 32, kernel_size=3, padding=1, l=l, k=k)\n",
    "        self.conv2 = LGRConv2d(32, 64, kernel_size=3, padding=1, l=l, k=k)\n",
    "        self.conv3 = LGRConv2d(64, 128, kernel_size=3, padding=1, l=l, k=k)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = LGRLinear(128 * 4 * 4, 128, l=l, k=k)\n",
    "        self.fc2 = nn.Linear(128, 10)  # Final layer can stay standard (no modulation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.conv1(x))\n",
    "        x = self.pool(self.conv2(x))\n",
    "        x = self.pool(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0f6c199-7f7d-4cd8-ac15-3b3b8de56a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b135b3c-4f87-4e9a-9f40-a31a75bf54b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_val = 0.01\n",
    "k_val = 10\n",
    "model = LGRCNN(l = l_val, k = k_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f6c53ea-6cae-4806-9ec4-ee0c747eafb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73699828-9ad9-4515-b051-59506a7fdd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1d0b653-3e2d-4a8b-80b1-141fdaf26d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                     "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'validate_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m logs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_grad_norm\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(grad_norms))\n\u001b[1;32m     66\u001b[0m logs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_weight_update_norm\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(weight_updates))\n\u001b[0;32m---> 68\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m(model,val_loader,device)\n\u001b[1;32m     70\u001b[0m logs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(val_loss)\n\u001b[1;32m     71\u001b[0m logs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(val_acc)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'validate_model' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    grad_norms = []\n",
    "    weight_updates = []\n",
    "    \n",
    "    logs = defaultdict(list)\n",
    "    running_loss = deque(maxlen=50)  # For smooth display\n",
    "    running_acc = deque(maxlen=50)\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "    for batch_idx, (inputs, labels) in enumerate(pbar):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "    \n",
    "        # Gradients and updates\n",
    "        total_norm = 0\n",
    "        weight_update_norm = 0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                total_norm += p.grad.data.norm(2).item() ** 2\n",
    "                weight_update_norm += (lr * p.grad.data).norm(2).item() ** 2\n",
    "    \n",
    "        grad_norms.append(np.sqrt(total_norm))\n",
    "        weight_updates.append(np.sqrt(weight_update_norm))\n",
    "    \n",
    "        optimizer.step()\n",
    "    \n",
    "        batch_size = inputs.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total += batch_size\n",
    "    \n",
    "        batch_acc = (outputs.argmax(1) == labels).float().mean().item()\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "    \n",
    "        # if batch_idx % log_every == 0:\n",
    "        #     logs['batches'].append({\n",
    "        #         'epoch': epoch,\n",
    "        #         'batch_idx': batch_idx,\n",
    "        #         'loss': loss.item(),\n",
    "        #         'acc': batch_acc,\n",
    "        #         'grad_norm': grad_norms[-1],\n",
    "        #         'weight_update_norm': weight_updates[-1],\n",
    "        #         'samples_seen': total\n",
    "        #     })\n",
    "    \n",
    "        # Track running stats for display\n",
    "        running_loss.append(loss.item())\n",
    "        running_acc.append(batch_acc)\n",
    "    \n",
    "        if batch_idx % 10 == 0:\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{np.mean(running_loss):.4f}',\n",
    "                'acc': f'{np.mean(running_acc) * 100:.2f}%',\n",
    "                'grad_norm': f'{grad_norms[-1]:.2f}'\n",
    "            })\n",
    "    \n",
    "    logs['train_loss'].append(total_loss / total)\n",
    "    logs['train_acc'].append(correct / total)\n",
    "    logs['train_grad_norm'].append(np.mean(grad_norms))\n",
    "    logs['train_weight_update_norm'].append(np.mean(weight_updates))\n",
    "    \n",
    "    val_loss, val_acc = validate_model(model,val_loader,device)\n",
    "    \n",
    "    logs['val_loss'].append(val_loss)\n",
    "    logs['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "          f\"Train Loss: {logs['train_loss'][-1]:.4f} | \"\n",
    "          f\"Train Acc: {logs['train_acc'][-1]*100:.2f}% | \"\n",
    "          f\"Val Acc: {logs['val_acc'][-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dca78b16-510f-48f4-ae8c-0e327267d0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 3, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict()['state'][0]['exp_avg_sq'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69cc7119-e3a2-4387-8255-bc45e7a35401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0156, 0.0022, 0.0042, 0.0110, 0.0039, 0.0025, 0.0014, 0.0112, 0.0022,\n",
       "        0.0028, 0.0008, 0.0027, 0.0037, 0.0033, 0.0014, 0.0030, 0.0073, 0.0040,\n",
       "        0.0021, 0.0040, 0.0156, 0.0012, 0.0142, 0.0096, 0.0014, 0.0066, 0.0032,\n",
       "        0.0020, 0.0011, 0.0008, 0.0027, 0.0146])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict()['state'][1]['exp_avg_sq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "353f89a6-949c-4679-bbe8-76da9de33ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr_linear = LGRLinear(128 * 4 * 4, 128, l=l_val, k=k_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c2ce5518-21e6-4417-b379-d1f70a98e847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0156, 0.0022, 0.0042, 0.0110, 0.0039, 0.0025, 0.0014, 0.0112, 0.0022,\n",
       "        0.0028, 0.0008, 0.0027, 0.0037, 0.0033, 0.0014, 0.0030, 0.0073, 0.0040,\n",
       "        0.0021, 0.0040, 0.0156, 0.0012, 0.0142, 0.0096, 0.0014, 0.0066, 0.0032,\n",
       "        0.0020, 0.0011, 0.0008, 0.0027, 0.0146])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict()['state'][1]['exp_avg_sq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "85c10aa0-1997-46d4-a513-49624c57d8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 3, 3])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['conv1.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cf2ea686-0612-4904-b2e3-44c2114e5b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conv1.weight',\n",
       " 'conv1.bias',\n",
       " 'conv2.weight',\n",
       " 'conv2.bias',\n",
       " 'conv3.weight',\n",
       " 'conv3.bias',\n",
       " 'fc1.weight',\n",
       " 'fc1.bias',\n",
       " 'fc2.weight',\n",
       " 'fc2.bias']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1fa5d765-9bbd-4bdb-9262-6f95c971bfa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.0175, -0.0126, -0.0215,  ..., -0.0125,  0.0165, -0.0206],\n",
       "                      [ 0.0028,  0.0102, -0.0090,  ..., -0.0056,  0.0098, -0.0034],\n",
       "                      [ 0.0016, -0.0030, -0.0073,  ...,  0.0173,  0.0062,  0.0142],\n",
       "                      ...,\n",
       "                      [-0.0081, -0.0192, -0.0060,  ..., -0.0066, -0.0157, -0.0179],\n",
       "                      [ 0.0097, -0.0153, -0.0042,  ...,  0.0203, -0.0189, -0.0202],\n",
       "                      [ 0.0160,  0.0105,  0.0142,  ..., -0.0122, -0.0215,  0.0133]])),\n",
       "             ('bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.]))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgr_linear.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f21c6d1-2815-4dfd-b79b-13589e519200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
